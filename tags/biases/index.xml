<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>biases on LC</title>
    <link>https://l1990790120.github.io/tags/biases/</link>
    <description>Recent content in biases on LC</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 02 Aug 2020 23:37:25 -0400</lastBuildDate>
    
	<atom:link href="https://l1990790120.github.io/tags/biases/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Thoughts on AI Biases</title>
      <link>https://l1990790120.github.io/post/2020-08-03-thoughts-on-ai-biases/</link>
      <pubDate>Sun, 02 Aug 2020 23:37:25 -0400</pubDate>
      
      <guid>https://l1990790120.github.io/post/2020-08-03-thoughts-on-ai-biases/</guid>
      <description>Almost in every AI or Machine Learning conferences I&amp;rsquo;ve been to lately, there&amp;rsquo;s a track dedicating to biases or &amp;ldquo;injustices&amp;rdquo; in algorithmic decisions. Books have been published (Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor, Algorithms of Oppression: How Search Engines Reinforce Racism etc.) and fear has been spread (Elon Musk says AI development should be better regulated, even at Tesla ).
The fear of unknown is, perhaps, more persuasive than a realistic survey of the state of AGI (Artificial General Intelligence) development.</description>
    </item>
    
  </channel>
</rss>